## Project Overview
--------------------------------------------------------------------------------------------------

In a microservices architecture, effective log management is crucial for operational excellence. This project aims to streamline the collection, storage, and analysis of logs generated by various services, enhancing the ability to track application behavior and identify errors quickly. By capturing relevant metadata alongside each log entry and enabling real-time ingestion and querying, the system improves operational visibility and facilitates proactive responses to potential issues. Ultimately, this distributed logging framework enhances resilience and maintainability in a dynamic application landscape.

[](https://hackmd.io/@pesu-bigdata/r1caCuFlkl#System-Architecture-and-Flow "System-Architecture-and-Flow")System Architecture and Flow
--------------------------------------------------------------------------------------------------------------------------------------

![Image 1: image](https://hackmd.io/_uploads/B1lNx_ce1g.png)

1.  **Microservices (Processes)**: Represent distributed nodes that independently generate logs and send heartbeat signals to monitor their status.
2.  **Log Accumulator**: Collects log data from each node, structures it, and forwards it to the Pub-Sub model for centralized log management.
3.  **Pub-Sub Model**: Acts as a communication layer, facilitating reliable, asynchronous distribution of logs.
4.  **Log Storage**: A system for indexing and storing logs in a searchable format for easy access and monitoring.
5.  **Alerting System**: Listens for specific log levels (e.g., ERROR, FATAL, etc) in real-time, generating alerts to ensure prompt responses to critical events.
6.  **Heartbeat Mechanism**: Provides failure detection by alerting when a node stops sending heartbeats, signaling that the node may have failed.

[](https://hackmd.io/@pesu-bigdata/r1caCuFlkl#Important- "Important-")Important :
---------------------------------------------------------------------------------

*   Implement at least three microservices or nodes, each capable of running as separate processes.  
    (It is not necessary to configure separate VMs, although you could do so.)
*   The log storage and alerting system must be centralized and hosted on a separate machine or virtual machine (VM).
    *   **Hint**: Set up Kafka on one machine to act as the log broker. Configure firewall and network settings to allow open communication. Each microservice can act as a producer, sending logs to Kafka, while the central log storage system acts as a consumer.
*   This central system should handle logs from all microservices, ensuring a unified place for storage, search, and alerting.

*   **Programming Language**:
    *   You are free to use any language such as Python, Go, Java, among others.
    *   Ensure that the chosen language supports all the required functionality.
    *   You are allowed to use any external libraries or APIs if required.
*   **Log Accumulator**: Fluentd or Apache Flume can be used.
*   **Pub-Sub Model**: Any pub-sub model like Apache Kafka, Rabbit/Active MQ.
*   **Log Storage**: Elasticsearch
*   **Visualisation (OPTIONAL)**: Kibana can be used to visualise stored logs.

[](https://hackmd.io/@pesu-bigdata/r1caCuFlkl#Log-Levels "Log-Levels")Log Levels
--------------------------------------------------------------------------------

```
​​​​INFO: General information about system operations.
​WARN: Indications of potential issues.
​ERROR: Errors that allow the system to continue running.
```

1.  Microservice Registration Message

```
{
    "node_id": "<unique node-id>",
    "message_type": "REGISTRATION",
    "service_name": "PaymentService, OrderService, etc",
    "timestamp": "<timestamp>"
}
```

2.  INFO Log

```
{
    "log_id": "<unique log-id>",
    "node_id": "<node-id>",
    "log_level": "INFO",
    "message_type": "LOG",
    "message": "<Log Message>",
    "service_name": "<ServiceName>",
    "timestamp": "<timestamp>"
}
```

3.  WARN Log

```
{
    "log_id": "<unique log-id>",
    "node_id": "<node-id>",
    "log_level": "WARN",
    "message_type": "LOG",
    "message": "",
    "service_name": "<ServiceName>",
    "response_time_ms": "",
    "threshold_limit_ms": "",
    "timestamp": "<timestamp>"
}
```

4.  ERROR Log

```
{
    "log_id": "<unique log-id>",
    "node_id": "<node-id>",
    "log_level": "ERROR",
    "message_type": "LOG",
    "message": "",
    "service_name": "<ServiceName>",
    "error_details": {
        "error_code": "",
        "error_message": ""
    },
    "timestamp": "<timestamp>"
}
```

5.  Heartbeat Message

```
{
    "node_id": "<node-id>",
    "message_type": "HEARTBEAT",
    "status": "UP",
    "timestamp": "<timestamp>"
}
```

6.  Microservice Registry

```
{
    "message_type":"REGISTRATION",
    "node_id": "<node-id>",
    "service_name": "<ServiceName>",
    "status": "UP/DOWN",
    "timestamp": "<timestamp>"
}
```

[](https://hackmd.io/@pesu-bigdata/r1caCuFlkl#CODE "CODE")CODE
--------------------------------------------------------------

1.  **Node Registration**
    
    *   Each microservice node should register with the system upon startup.
    *   Ensure each node has a **unique ID** for identification in logs and heartbeats.
2.  **Heartbeat Mechanism**
    
    *   Each node sends a **heartbeat message** at fixed intervals to confirm it’s active.
3.  **Log Generation and Accumulation**
    
    *   Nodes generate logs of various levels (e.g., INFO, WARN, ERROR).
    *   Logs should pass through a **Log Accumulator** within each node.
    *   Accumulators format logs for the **Pub-Sub Model**, ensuring they are either sent in real-time or batched as needed.
4.  **Indexing and Storage**
    
    *   Logs sent through the Pub-Sub Model are stored in **Log Storage**.
    *   Logs should be indexed by suitable criterias for efficent log retrieval and querying.
5.  **Critical Log Alerts**
    
    *   Any log with a level of **ERROR** or **WARN** should trigger an alert in the terminal or UI.
6.  **Node Failure Detection**
    
    *   If the **Heartbeat Monitor** detects a missing signal from a node, it should trigger an alert.
7.  **Visualisation (OPTIONAL)**
    

*   **Kibana** can be used to visualize stored logs.
    *   Possible visualizations:
        *   **Node Uptime**: Track heartbeat signals and uptime for each node.
        *   **Critical Log Counts**: Display the number of ERROR or FATAL logs generated by each node.
        *   **Log Frequency**: Monitor log frequency to identify spikes and potential issues.
        *   **Node Activity**: Show active vs. inactive nodes for system health monitoring.

[](https://hackmd.io/@pesu-bigdata/r1caCuFlkl#Weekly-Commit-Milestones "Weekly-Commit-Milestones")Weekly Commit Milestones
--------------------------------------------------------------------------------------------------------------------------

### [](https://hackmd.io/@pesu-bigdata/r1caCuFlkl#Week-1-Microservice-and-Log-Generation-Setup "Week-1-Microservice-and-Log-Generation-Setup")Week 1: Microservice and Log Generation Setup

*   **Microservices Configuration**: Set up at least three separate processes as independent nodes. Each node generates logs autonomously, and you may run these on multiple VMs or a single machine for simplicity.
*   **Log Generation**: Implement each node to create logs of varying severity (INFO, WARN, ERROR).
*   **Heartbeat Mechanism**: Establish a basic heartbeat system where each node sends periodic signals to indicate it’s active. Missing heartbeats should trigger alerts to simulate node failure detection.

**Deliverables for Week 1**:

*   Configured nodes generating logs and heartbeat signals.

### [](https://hackmd.io/@pesu-bigdata/r1caCuFlkl#Week-2-Log-Accumulator-and-Pub-Sub-Model-Integration "Week-2-Log-Accumulator-and-Pub-Sub-Model-Integration")Week 2: Log Accumulator and Pub-Sub Model Integration

*   **Log Accumulator Configuration**
*   **Pub-Sub Model Setup**
*   **Subscriber Configuration**

**Deliverables for Week 2**:

*   Functional log accumulator configured to collect, structure and forward logs to the Pub-Sub model.
*   Pub-Sub model set up to manage logs from multiple nodes.

### [](https://hackmd.io/@pesu-bigdata/r1caCuFlkl#Week-3-Alerting-Log-Storage-and-Final-Integration "Week-3-Alerting-Log-Storage-and-Final-Integration")Week 3: Alerting, Log Storage, and Final Integration

*   **Alerting System**
*   **Log Storage Configuration**: Ensure that logs are stored and indexed using Elasticsearch, allowing them to be queried by fields and attributes for quick retrieval.
*   **Final Testing and Validation**: Verify the complete pipeline—from log generation to storage and alerting—ensuring all components communicate as expected. Test the heartbeat detection for effective failure monitoring.

**Deliverables for Week 3**:

*   A working alerting system for detecting and responding to critical logs.
*   Fully operational log storage for storing and retrieving logs effectively.

[](https://hackmd.io/@pesu-bigdata/r1caCuFlkl#References "References")References
--------------------------------------------------------------------------------

*   [Distributed Logging System Design on Medium](https://medium.com/@krrishan7495/distributed-logging-designing-a-robust-system-for-enhanced-monitoring-64ddb0838882)
*   [Distributed Logging Architecture on DZone](https://dzone.com/articles/distributed-logging-architecture-for-microservices)
*   [Remote Kafka Connection Guide by Bitnami](https://docs.bitnami.com/google-templates/infrastructure/kafka/administration/connect-remotely/)
*   [Fluentd Output to Kafka Documentation](https://docs.fluentd.org/0.12/output/kafka)
*   [Python Client for Elasticsearch - Getting Started Guide](https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/getting-started-python.html)
*   [ngrok](https://ngrok.com/docs/guides/device-gateway/linux/)

* * *